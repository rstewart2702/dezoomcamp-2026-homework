## Homework 3
### Loading Data
Python to load into my GCP Big Query tables is [here](03-data-warehouse/load_yellow_taxi_data.py).


### Homework and the Queries Used
Question 1. Counting records

What is count of records for the 2024 Yellow Taxi Data?

    65,623
    840,402
    20,332,093
    85,431,289

ANSWER:

    20,332,093

WORK:
```
select count(*) from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_tripdata`;
```

returned a count of:

20332093

=========================================================================

Question 2. Data read estimation

Write a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.

What is the estimated amount of data that will be read when this query is executed on the External Table and the Table?

    18.82 MB for the External Table and 47.60 MB for the Materialized Table
    0 MB for the External Table and 155.12 MB for the Materialized Table
    2.14 GB for the External Table and 0MB for the Materialized Table
    0 MB for the External Table and 0MB for the Materialized Table

ANSWER:

    0 MB for the External Table and 155.12 MB for the Materialized Table

WORK:

Concluded this by highlighting the SQL statement in the web-page editor
and looking for a "green, circular checkmark icon" and message to appear
below the query-editor pane; the message for the query of the non-partitioned
"regular bigquery table" named "fhv_nonpartitioned_tripdata" was:

  `This query will process 155.12 MB when run.`

=========================================================================

Question 3. Understanding columnar storage

Write a query to retrieve the PULocationID from the table (not the external table) in BigQuery. Now write a query to retrieve the PULocationID and DOLocationID on the same table.

Why are the estimated number of Bytes different?

    BigQuery is a columnar database, and it only scans the specific columns requested in the query. Querying two columns (PULocationID, DOLocationID) requires reading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed.
    BigQuery duplicates data across multiple storage partitions, so selecting two columns instead of one requires scanning the table twice, doubling the estimated bytes processed.
    BigQuery automatically caches the first queried column, so adding a second column increases processing time but does not affect the estimated bytes scanned.
    When selecting multiple columns, BigQuery performs an implicit join operation between them, increasing the estimated bytes processed

ANSWER:

    BigQuery is a columnar database, and it only scans the specific columns requested in the query. Querying two columns (PULocationID, DOLocationID) requires reading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed.

WORK:

I shall verify by looking at the estimates published for both of the requested queries, but
the answer is that the query which mentions two separate columns scans more data, because
the columns are stored separately; this implies that the more columns you access in a columnar
database, the more data you can end up scanning, ceteris paribus.

Here are the two queries with the "data read estimates" Google's service provided:

```
select t.PULocationID from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_nonpartitioned_tripdata` t;
```

Provided estimate was:

`This query will process 155.12 MB when run.`

```
select t.PULocationID, t.DOLocationID from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_nonpartitioned_tripdata` t;
```

Provided estimate was:

`This query will process 310.24 MB when run.`

=========================================================================

Question 4. Counting zero fare trips

How many records have a fare_amount of 0?

    128,210
    546,578
    20,188,016
    8,333

ANSWER:

    8,333

WORK:
```
select count(*) from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_nonpartitioned_tripdata` t where t.fare_amount = 0;
```

returned the count of 8333.

=========================================================================

Question 5. Partitioning and clustering

What is the best strategy to make an optimized table in Big Query if your query will always filter based on tpep_dropoff_datetime and order the results by VendorID (Create a new table with this strategy)

    Partition by tpep_dropoff_datetime and Cluster on VendorID
    Cluster on by tpep_dropoff_datetime and Cluster on VendorID
    Cluster on tpep_dropoff_datetime Partition by VendorID
    Partition by tpep_dropoff_datetime and Partition by VendorID

ANSWER:

    Partition by tpep_dropoff_datetime and Cluster on VendorID

WORK:

My first inclination was to answer:
    Cluster on by tpep_dropoff_datetime and Cluster on VendorID

But then the exercise showed that one literlly may not partition on a
timestamp valued column; on must compute a function of it, such as
extracting the date-portion of it.  When doing that, then the table
becomes, in this case, 192 "mini-tables," each of which is pre-sorted
on the VendorID (probably because the rows literally are ordered on that
column, or there is a b-tree index for the mini-table on that value,
or else the entire mini-table structure IS organized in a btree structure!)

So, the better, possible-in-Big-Query answer is:

    Partition by tpep_dropoff_datetime and Cluster on VendorID

TABLE-CREATION STATEMENTS:
```
create table `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_clustered_tripdata`
cluster by tpep_dropoff_datetime, VendorID
as select * from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_tripdata` ;
```

```
create table `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_partitioned_tripdata`
partition by date(tpep_dropoff_datetime)
cluster by tpep_dropoff_datetime, VendorID
as select * from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_tripdata` ;
```

```
select t.*
from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_partitioned_tripdata` t
where timestamp("2024-06-01")  <= t.tpep_dropoff_datetime and t.tpep_dropoff_datetime < timestamp("2024-06-02")
order by t.VendorID;
-- estimated 17.66 MB traversed 
```

```
select t.*
from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_partitioned_tripdata` t
where timestamp('2024-06-01') = date_trunc(t.tpep_dropoff_datetime,DAY)
order by t.VendorID;
-- estimated 17.66 MB traversed 
```

```
select t.*
from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_clustered_tripdata` t
where timestamp("2024-06-01")  <= t.tpep_dropoff_datetime and t.tpep_dropoff_datetime < timestamp("2024-06-02")
order by t.VendorID;
-- estimated 103.51 MB traversed

/* this means that partitioning can SIGNIFICANTLY reduce the amount of data traversed,
   when the partitioning is possible. */
```


I queried for the cardinality of the tpsp_dropoff_datetime column, and discovered
that it has almost 1e+11 (that is 10 MILLION) distinct values.  This number is far
higher than the maximum number of permitted partitions allowed for a Big Query
partitioned table.  Therefore, the best one could do would be to cluster first on
tpep_dropoff_datetime, ensuring that filtering operations will be cheap, and also
to cluster on VendorID, which will make it cheap to impose ordering by VendorID on
the result set of rows retrieved.

Any other choice in the list of choices will either lead to a failure to partition
(because there are far more possible values than the number of allowed partitions)
or will lead to more i/o than the choice chosen for the answer, either because
the partitioning will not be useful (due to the stated filtering and ordering
contraint...)

What they did NOT make clear is whether or not we want to partition on the
date portion of the tpep_dropoff_datetime.  If we do that, then there are only 192
distinct days in question, so this makes it much less clear what to do.  That said,

If we are trying to filter on a particular DAY (ignoring the timestamp portion) then
it makes sense to partition by the DATE(tpep_dropoff_datetime) and then cluster
based on VendorID, because the filtering can be done by the paritioning VERY CHEAPLY,
and the clustering ensures that the data will already be sorted.

On the other hand, if we are filtering on the entirety of the tpep_dropoff_datetime,
then partitioning on tpep_dropoff_datetime is impossible, and the best we could
do would be to cluster based on tpep_dropoff_datetime, VendorID.

=========================================================================

Question 6. Partition benefits

Write a query to retrieve the distinct VendorIDs between tpep_dropoff_datetime 2024-03-01 and 2024-03-15 (inclusive)

Use the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 5 and note the estimated bytes processed. What are these values?

Choose the answer which most closely matches.

    12.47 MB for non-partitioned table and 326.42 MB for the partitioned table
    310.24 MB for non-partitioned table and 26.84 MB for the partitioned table
    5.87 MB for non-partitioned table and 0 MB for the partitioned table
    310.31 MB for non-partitioned table and 285.64 MB for the partitioned table

ANSWER:

    310.24 MB for non-partitioned table and 26.84 MB for the partitioned table

WORK:
```
select distinct(t.VendorId) distinct_vendorid
from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_nonpartitioned_tripdata` t
where timestamp('2024-03-01') <= t.tpep_dropoff_datetime and t.tpep_dropoff_datetime < timestamp('2024-03-16')
-- esimtated 310.24 MB to be processed/traversed.
```

```
select distinct(t.VendorId) distinct_vendorid
from `datatalks-dezoomcamp2026.datatalks_dezoomcamp2026_dataset.fhv_partitioned_tripdata` t
where timestamp('2024-03-01') <= t.tpep_dropoff_datetime and t.tpep_dropoff_datetime < timestamp('2024-03-16')
-- estimated 26.84 MB to be processed/traversed.
```

=========================================================================

Question 7. External table storage

Where is the data stored in the External Table you created?

    Big Query
    Container Registry
    GCP Bucket
    Big Table

ANSWER:

    GCP Bucket

=========================================================================

Question 8. Clustering best practices

It is best practice in Big Query to always cluster your data:

    True
    False

ANSWER:

    False

WORK:

Clustering can impose overhead, extra processing work-and-cost; it depends on the
kind of querying that will be done, i.e., the "workload" that will be needed.

=========================================================================

Question 9. Understanding table scans

No Points: Write a SELECT count(*) query FROM the materialized table you created. How many bytes does it estimate will be read? Why?

ANSWER:

    Big Query's estimate is "0 B when run."

My initial guess was that was likely because some of this information is already cached.
    
When I looked at the execution-plan-graph, it was pretty clear that, due to the way Big Query
works, record tallies of that kind are stored in readily-available locations like system-catalogs
or information-schemata, because something like Big Query is a "read-mostly" or "read-only"
environment, and in such an environment, the total number of rows in a table is something that is
statically known ahead of all queries.

=========================================================================




